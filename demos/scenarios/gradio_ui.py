#!/usr/bin/env python
# gradio_ui.py

import sys
import os
import time
import datetime
sys.path.insert(0, "../../")  # è°ƒæ•´ä¸ºæ­£ç¡®çš„é¡¹ç›®æ ¹è·¯å¾„
import gradio as gr
import yaml
import json
from dotenv import load_dotenv

# å¯¼å…¥ä¸‰ä¸ªåœºæ™¯
from scenario1_no_data_no_examples import run_scenario1
from scenario2_no_data_synthetic_examples import run_scenario2 
from scenario3_with_data_with_examples import run_scenario3

# åŠ è½½é…ç½®æ–‡ä»¶
def load_yaml_config(file_path):
    try:
        with open(file_path, 'r', encoding='utf-8') as file:
            return yaml.safe_load(file)
    except Exception as e:
        print(f"åŠ è½½é…ç½®æ–‡ä»¶å‡ºé”™: {e}")
        return {}

# å°†å­—å…¸è½¬æ¢ä¸ºYAMLæ ¼å¼å­—ç¬¦ä¸²
def dict_to_yaml_str(config_dict):
    if not config_dict:
        return ""
    try:
        return yaml.dump(config_dict, allow_unicode=True)
    except Exception as e:
        print(f"è½¬æ¢YAMLå‡ºé”™: {e}")
        return ""

def save_yaml_config(file_path, config_dict):
    try:
        with open(file_path, 'w', encoding='utf-8') as file:
            yaml.dump(config_dict, file, allow_unicode=True)
        return True, f"é…ç½®å·²ä¿å­˜åˆ° {file_path}"
    except Exception as e:
        return False, f"ä¿å­˜é…ç½®å¤±è´¥: {e}"

def load_configs():
    promptopt_config = load_yaml_config("configs/promptopt_config.yaml")
    llm_config = load_yaml_config("configs/llm_config.yaml")
    setup_config = load_yaml_config("configs/setup_config.yaml")
    return promptopt_config, llm_config, setup_config

# ç»“æœä¿å­˜å‡½æ•°
def save_result(best_prompt, expert_profile, experiment_name="experiment"):
    """ä¿å­˜ä¼˜åŒ–ç»“æœ"""
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    results_dir = "results"
    
    # å¦‚æœç›®å½•ä¸å­˜åœ¨ï¼Œåˆ›å»ºå®ƒ
    if not os.path.exists(results_dir):
        os.makedirs(results_dir)
    
    # æ–‡ä»¶åæ ¼å¼
    filename = f"{results_dir}/{experiment_name}_{timestamp}.json"
    
    # ä¿å­˜ç»“æœ
    result_data = {
        "timestamp": timestamp,
        "experiment_name": experiment_name,
        "best_prompt": best_prompt,
        "expert_profile": expert_profile
    }
    
    try:
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(result_data, f, ensure_ascii=False, indent=2)
        return f"ç»“æœå·²ä¿å­˜åˆ° {filename}"
    except Exception as e:
        return f"ä¿å­˜ç»“æœå¤±è´¥: {e}"

# æ—¥å¿—æ›´æ–°å‡½æ•°
def update_log(log_box, message):
    """æ›´æ–°æ—¥å¿—æ˜¾ç¤º"""
    timestamp = datetime.datetime.now().strftime("%H:%M:%S")
    return log_box + f"\n[{timestamp}] {message}"

# åˆ›å»ºåˆæˆæ•°æ®
def create_synthetic_data(log_output):
    """åˆ›å»ºåˆæˆè®­ç»ƒæ•°æ®"""
    log_output = update_log(log_output, "å¼€å§‹åˆ›å»ºåˆæˆæ•°æ®...")
    
    try:
        # è¿™é‡Œå¯ä»¥è°ƒç”¨åˆ›å»ºåˆæˆæ•°æ®çš„å‡½æ•°
        # å‡è®¾ä½¿ç”¨åœºæ™¯2çš„éƒ¨åˆ†åŠŸèƒ½
        log_output = update_log(log_output, "æ­£åœ¨è°ƒç”¨åˆæˆæ•°æ®ç”Ÿæˆå‡½æ•°...")
        # TODO: å®ç°åˆæˆæ•°æ®ç”ŸæˆåŠŸèƒ½
        
        time.sleep(1)  # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´
        log_output = update_log(log_output, "åˆæˆæ•°æ®åˆ›å»ºå®Œæˆ")
        return log_output, "train_synthetic.jsonl"
    except Exception as e:
        log_output = update_log(log_output, f"åˆ›å»ºåˆæˆæ•°æ®æ—¶å‡ºé”™: {e}")
        return log_output, None

# è¿è¡Œåœºæ™¯1å‡½æ•°
def run_scenario1_ui(task_description, base_instruction, model_id, mutation_rounds, use_expert_identity, log_output):
    # è®°å½•å¼€å§‹
    log_output = update_log(log_output, f"å¼€å§‹è¿è¡Œåœºæ™¯1: æ²¡æœ‰è®­ç»ƒæ•°æ®ï¼Œä¸ä½¿ç”¨æ ·ä¾‹")
    log_output = update_log(log_output, f"ä½¿ç”¨æ¨¡å‹: {model_id}")
    
    try:
        # è®¾ç½®é…ç½®
        os.environ["OPENAI_API_KEY"] = api_key
        
        # å®šä¹‰é…ç½®å‚æ•°
        promptopt_config = {
            "task_description": task_description,
            "base_instruction": base_instruction,
            "mutation_rounds": int(mutation_rounds),
            "max_history_length": 10,
            "use_examples_in_context": False,
            "generate_expert_identity": use_expert_identity,
            "prompt_templates_file": "configs/prompt_library.yaml"
        }
        
        setup_config = {
            "experiment_name": "no_data_no_examples",
            "experiment_id": f"scenario1_{int(time.time())}",
            "model_id": model_id
        }
        
        # è®°å½•é…ç½®
        log_output = update_log(log_output, f"é…ç½®å·²è®¾ç½®ï¼Œå¼€å§‹ä¼˜åŒ–æç¤º...")
        
        # æ‰§è¡Œåœºæ™¯1
        best_prompt, expert_profile = run_scenario1()
        
        # ä¿å­˜ç»“æœ
        save_msg = save_result(best_prompt, expert_profile, setup_config["experiment_name"])
        log_output = update_log(log_output, save_msg)
        
        return f"### ä¸“å®¶æ¡£æ¡ˆ\n{expert_profile}\n\n### æœ€ä½³æç¤º\n{best_prompt}", log_output
    except Exception as e:
        error_msg = f"åœºæ™¯1æ‰§è¡Œå‡ºé”™: {e}"
        log_output = update_log(log_output, error_msg)
        return error_msg, log_output

# è¿è¡Œåœºæ™¯2å‡½æ•°
def run_scenario2_ui(task_description, base_instruction, model_id, mutation_rounds, num_examples, log_output):
    # è®°å½•å¼€å§‹
    log_output = update_log(log_output, f"å¼€å§‹è¿è¡Œåœºæ™¯2: æ²¡æœ‰è®­ç»ƒæ•°æ®ï¼Œä½¿ç”¨åˆæˆæ ·ä¾‹")
    log_output = update_log(log_output, f"ä½¿ç”¨æ¨¡å‹: {model_id}, ç”Ÿæˆ{num_examples}ä¸ªæ ·ä¾‹")
    
    try:
        # è®¾ç½®é…ç½®
        os.environ["OPENAI_API_KEY"] = api_key
        
        # æ‰§è¡Œåœºæ™¯2
        best_prompt, expert_profile = run_scenario2()
        
        # ä¿å­˜ç»“æœ
        save_msg = save_result(best_prompt, expert_profile, "no_data_synthetic_examples")
        log_output = update_log(log_output, save_msg)
        
        return f"### ä¸“å®¶æ¡£æ¡ˆ\n{expert_profile}\n\n### æœ€ä½³æç¤º\n{best_prompt}", log_output
    except Exception as e:
        error_msg = f"åœºæ™¯2æ‰§è¡Œå‡ºé”™: {e}"
        log_output = update_log(log_output, error_msg)
        return error_msg, log_output

# è¿è¡Œåœºæ™¯3å‡½æ•°
def run_scenario3_ui(task_description, base_instruction, model_id, dataset_path, few_shot_count, log_output):
    # è®°å½•å¼€å§‹
    log_output = update_log(log_output, f"å¼€å§‹è¿è¡Œåœºæ™¯3: æœ‰è®­ç»ƒæ•°æ®ï¼Œä½¿ç”¨æ ·ä¾‹")
    log_output = update_log(log_output, f"ä½¿ç”¨æ¨¡å‹: {model_id}, æ•°æ®é›†: {dataset_path}")
    
    try:
        # è®¾ç½®é…ç½®
        os.environ["OPENAI_API_KEY"] = api_key
        
        # æ‰§è¡Œåœºæ™¯3
        best_prompt, expert_profile = run_scenario3()
        
        # ä¿å­˜ç»“æœ
        save_msg = save_result(best_prompt, expert_profile, "with_data_with_examples")
        log_output = update_log(log_output, save_msg)
        
        return f"### ä¸“å®¶æ¡£æ¡ˆ\n{expert_profile}\n\n### æœ€ä½³æç¤º\n{best_prompt}", log_output
    except Exception as e:
        error_msg = f"åœºæ™¯3æ‰§è¡Œå‡ºé”™: {e}"
        log_output = update_log(log_output, error_msg)
        return error_msg, log_output

# ä¿å­˜APIå¯†é’¥
api_key = ""
def save_api_key(key):
    global api_key
    api_key = key
    return "APIå¯†é’¥å·²ä¿å­˜"

# ä¿å­˜é…ç½®æ–‡ä»¶
def save_promptopt_config(config_content):
    try:
        # è§£æYAMLå†…å®¹
        config_dict = yaml.safe_load(config_content)
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        success, message = save_yaml_config("configs/promptopt_config.yaml", config_dict)
        
        return message
    except Exception as e:
        return f"ä¿å­˜PromptOpté…ç½®æ–‡ä»¶å¤±è´¥: {e}"

def save_llm_config(config_content):
    try:
        # è§£æYAMLå†…å®¹
        config_dict = yaml.safe_load(config_content)
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        success, message = save_yaml_config("configs/llm_config.yaml", config_dict)
        
        return message
    except Exception as e:
        return f"ä¿å­˜LLMé…ç½®æ–‡ä»¶å¤±è´¥: {e}"

def save_prompt_library(config_content):
    try:
        # è§£æYAMLå†…å®¹
        config_dict = yaml.safe_load(config_content)
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        success, message = save_yaml_config("configs/prompt_library.yaml", config_dict)
        
        return message
    except Exception as e:
        return f"ä¿å­˜æç¤ºè¯æ¨¡æ¿æ–‡ä»¶å¤±è´¥: {e}"

# åˆ›å»ºGradioç•Œé¢
with gr.Blocks(title="PromptWizardä¼˜åŒ–å™¨", theme=gr.themes.Soft()) as demo:
    gr.Markdown("# ğŸ§™â€â™‚ï¸ PromptWizardæç¤ºè¯ä¼˜åŒ–ç³»ç»Ÿ")
    gr.Markdown("æç¤ºè¯ä¼˜åŒ–ç³»ç»Ÿå¯ä»¥å¸®åŠ©ä¼˜åŒ–å¤§è¯­è¨€æ¨¡å‹çš„æç¤ºè¯ï¼Œæå‡æ¨¡å‹çš„è¡¨ç°å’Œè¾“å‡ºè´¨é‡ã€‚")
    
    # APIå¯†é’¥è®¾ç½®
    with gr.Accordion("ğŸ”‘ APIè®¾ç½®", open=True):
        api_key_input = gr.Textbox(label="OpenAI APIå¯†é’¥", type="password")
        save_key_btn = gr.Button("ä¿å­˜APIå¯†é’¥")
        key_status = gr.Textbox(label="çŠ¶æ€")
        save_key_btn.click(save_api_key, inputs=api_key_input, outputs=key_status)
    
    # æ—¥å¿—è¾“å‡º
    log_output = gr.Textbox(label="è¿è¡Œæ—¥å¿—", lines=10, max_lines=30, interactive=False)
    
    # åœºæ™¯é€‰æ‹©æ ‡ç­¾é¡µ
    with gr.Tabs():
        # åœºæ™¯1ï¼šæ²¡æœ‰è®­ç»ƒæ•°æ®ï¼Œä¸ä½¿ç”¨æ ·ä¾‹
        with gr.Tab("åœºæ™¯1: æ²¡æœ‰è®­ç»ƒæ•°æ®ï¼Œä¸ä½¿ç”¨æ ·ä¾‹"):
            with gr.Row():
                with gr.Column(scale=2):
                    task_desc1 = gr.Textbox(
                        label="ä»»åŠ¡æè¿°", 
                        value="You are a mathematics expert. You will be given a mathematics problem which you need to solve",
                        lines=3
                    )
                    base_instr1 = gr.Textbox(
                        label="åŸºç¡€æŒ‡ä»¤", 
                        value="Lets think step by step.",
                        lines=2
                    )
                    
                    with gr.Row():
                        model_id1 = gr.Dropdown(
                            label="æ¨¡å‹ID", 
                            choices=["gpt-4o", "gpt-3.5-turbo", "Pro/deepseek-ai/DeepSeek-V3"], 
                            value="gpt-4o"
                        )
                        mutation_rounds1 = gr.Slider(
                            label="å˜å¼‚è½®æ¬¡", 
                            minimum=1, 
                            maximum=10, 
                            value=5, 
                            step=1
                        )
                    
                    expert_identity1 = gr.Checkbox(
                        label="ç”Ÿæˆä¸“å®¶èº«ä»½", 
                        value=True
                    )
                    
                    run_btn1 = gr.Button("è¿è¡Œåœºæ™¯1", variant="primary")
                
                with gr.Column(scale=3):
                    result1 = gr.Markdown(label="ç»“æœ")
            
            run_btn1.click(
                run_scenario1_ui, 
                inputs=[task_desc1, base_instr1, model_id1, mutation_rounds1, expert_identity1, log_output], 
                outputs=[result1, log_output]
            )
        
        # åœºæ™¯2ï¼šæ²¡æœ‰è®­ç»ƒæ•°æ®ï¼Œä½¿ç”¨åˆæˆæ ·ä¾‹
        with gr.Tab("åœºæ™¯2: æ²¡æœ‰è®­ç»ƒæ•°æ®ï¼Œä½¿ç”¨åˆæˆæ ·ä¾‹"):
            with gr.Row():
                with gr.Column(scale=2):
                    task_desc2 = gr.Textbox(
                        label="ä»»åŠ¡æè¿°", 
                        value="You are a mathematics expert. You will be given a mathematics problem which you need to solve",
                        lines=3
                    )
                    base_instr2 = gr.Textbox(
                        label="åŸºç¡€æŒ‡ä»¤", 
                        value="Lets think step by step.",
                        lines=2
                    )
                    
                    with gr.Row():
                        model_id2 = gr.Dropdown(
                            label="æ¨¡å‹ID", 
                            choices=["gpt-4o", "gpt-3.5-turbo", "Pro/deepseek-ai/DeepSeek-V3"], 
                            value="gpt-4o"
                        )
                        mutation_rounds2 = gr.Slider(
                            label="å˜å¼‚è½®æ¬¡", 
                            minimum=1, 
                            maximum=10, 
                            value=2, 
                            step=1
                        )
                    
                    num_examples2 = gr.Slider(
                        label="ç”Ÿæˆæ ·ä¾‹æ•°é‡", 
                        minimum=5, 
                        maximum=30, 
                        value=20, 
                        step=5
                    )
                    
                    create_data_btn = gr.Button("åˆ›å»ºåˆæˆæ•°æ®")
                    synthetic_data_path = gr.Textbox(label="åˆæˆæ•°æ®è·¯å¾„", value="")
                    
                    run_btn2 = gr.Button("è¿è¡Œåœºæ™¯2", variant="primary")
                
                with gr.Column(scale=3):
                    result2 = gr.Markdown(label="ç»“æœ")
            
            create_data_btn.click(
                create_synthetic_data,
                inputs=[log_output],
                outputs=[log_output, synthetic_data_path]
            )
            
            run_btn2.click(
                run_scenario2_ui, 
                inputs=[task_desc2, base_instr2, model_id2, mutation_rounds2, num_examples2, log_output], 
                outputs=[result2, log_output]
            )
        
        # åœºæ™¯3ï¼šæœ‰è®­ç»ƒæ•°æ®ï¼Œä½¿ç”¨æ ·ä¾‹
        with gr.Tab("åœºæ™¯3: æœ‰è®­ç»ƒæ•°æ®ï¼Œä½¿ç”¨æ ·ä¾‹"):
            with gr.Row():
                with gr.Column(scale=2):
                    task_desc3 = gr.Textbox(
                        label="ä»»åŠ¡æè¿°", 
                        value="You are a mathematics expert. You will be given a mathematics problem which you need to solve",
                        lines=3
                    )
                    base_instr3 = gr.Textbox(
                        label="åŸºç¡€æŒ‡ä»¤", 
                        value="Lets think step by step.",
                        lines=2
                    )
                    
                    with gr.Row():
                        model_id3 = gr.Dropdown(
                            label="æ¨¡å‹ID", 
                            choices=["gpt-4o", "gpt-3.5-turbo", "Pro/deepseek-ai/DeepSeek-V3"], 
                            value="Pro/deepseek-ai/DeepSeek-V3"
                        )
                        few_shot_count3 = gr.Slider(
                            label="å°‘æ ·æœ¬æ•°é‡", 
                            minimum=1, 
                            maximum=10, 
                            value=5, 
                            step=1
                        )
                    
                    dataset_path3 = gr.Textbox(
                        label="è®­ç»ƒæ•°æ®è·¯å¾„", 
                        value="train_synthetic.jsonl"
                    )
                    
                    run_btn3 = gr.Button("è¿è¡Œåœºæ™¯3", variant="primary")
                
                with gr.Column(scale=3):
                    result3 = gr.Markdown(label="ç»“æœ")
            
            run_btn3.click(
                run_scenario3_ui, 
                inputs=[task_desc3, base_instr3, model_id3, dataset_path3, few_shot_count3, log_output], 
                outputs=[result3, log_output]
            )
    
    # é«˜çº§é…ç½®æ ‡ç­¾é¡µ
    with gr.Tabs():
        with gr.Tab("PromptOpté…ç½®"):
            # åŠ è½½é…ç½®å¹¶è½¬æ¢ä¸ºYAMLå­—ç¬¦ä¸²
            promptopt_config_yaml_dict = load_yaml_config("configs/promptopt_config.yaml")
            promptopt_config_yaml_value = dict_to_yaml_str(promptopt_config_yaml_dict)
            if not promptopt_config_yaml_value:
                promptopt_config_yaml_value = """# é«˜çº§PromptOpté…ç½®
task_description: "You are a mathematics expert. You will be given a mathematics problem which you need to solve"
base_instruction: "Lets think step by step."
answer_format: "For each question present the reasoning followed by the correct answer."
mutation_rounds: 2
few_shot_count: 5
generate_expert_identity: true
generate_intent_keywords: false
generate_reasoning: true
mutate_refine_iterations: 3
min_correct_count: 3
max_eval_batches: 6
num_train_examples: 20
prompt_technique_name: "critique_n_refine"
questions_batch_size: 1
refine_instruction: true
refine_task_eg_iterations: 3
seen_set_size: 20
style_variation: 5
top_n: 1
unique_model_id: "Pro/deepseek-ai/DeepSeek-V3"
max_history_length: 10"""
            
            promptopt_config_yaml = gr.Code(
                label="PromptOpté…ç½®", 
                language="yaml", 
                value=promptopt_config_yaml_value
            )
            save_promptopt_btn = gr.Button("ä¿å­˜PromptOpté…ç½®")
            promptopt_save_status = gr.Textbox(label="ä¿å­˜çŠ¶æ€")
            
            save_promptopt_btn.click(
                save_promptopt_config,
                inputs=promptopt_config_yaml,
                outputs=promptopt_save_status
            )
            
        with gr.Tab("LLMé…ç½®"):
            # åŠ è½½é…ç½®å¹¶è½¬æ¢ä¸ºYAMLå­—ç¬¦ä¸²
            llm_config_yaml_dict = load_yaml_config("configs/llm_config.yaml")
            llm_config_yaml_value = dict_to_yaml_str(llm_config_yaml_dict)
            if not llm_config_yaml_value:
                llm_config_yaml_value = """# LLMé…ç½®
model_id: "gpt-4o"
temperature: 0.7
max_tokens: 1000"""
            
            llm_config_yaml = gr.Code(
                label="LLMé…ç½®", 
                language="yaml",
                value=llm_config_yaml_value
            )
            save_llm_btn = gr.Button("ä¿å­˜LLMé…ç½®")
            llm_save_status = gr.Textbox(label="ä¿å­˜çŠ¶æ€")
            
            save_llm_btn.click(
                save_llm_config,
                inputs=llm_config_yaml,
                outputs=llm_save_status
            )
            
        with gr.Tab("æç¤ºè¯æ¨¡æ¿"):
            # åŠ è½½é…ç½®å¹¶è½¬æ¢ä¸ºYAMLå­—ç¬¦ä¸²
            prompt_library_yaml_dict = load_yaml_config("configs/prompt_library.yaml")
            prompt_library_yaml_value = dict_to_yaml_str(prompt_library_yaml_dict)
            if not prompt_library_yaml_value:
                prompt_library_yaml_value = """# æç¤ºè¯æ¨¡æ¿åº“
system_prompts: |
  You are a helpful assistant that assists research students in understanding research papers.
system_guidelines: |
  Guidelines 
  - Your role must always be a helpful assistant that assists students in understanding research papers.
  - Only answer questions that are directly or indirectly related to the referenced paper(s)."""
            
            prompt_library_yaml = gr.Code(
                label="æç¤ºè¯æ¨¡æ¿åº“", 
                language="yaml",
                value=prompt_library_yaml_value
            )
            save_prompt_library_btn = gr.Button("ä¿å­˜æç¤ºè¯æ¨¡æ¿")
            prompt_library_save_status = gr.Textbox(label="ä¿å­˜çŠ¶æ€")
            
            save_prompt_library_btn.click(
                save_prompt_library,
                inputs=prompt_library_yaml,
                outputs=prompt_library_save_status
            )
    
    # å¸®åŠ©å’Œè¯´æ˜
    with gr.Accordion("ğŸ“– ä½¿ç”¨è¯´æ˜", open=False):
        gr.Markdown("""
        ## PromptWizard ä½¿ç”¨è¯´æ˜
        
        ### åœºæ™¯1ï¼šæ²¡æœ‰è®­ç»ƒæ•°æ®ï¼Œä¸ä½¿ç”¨æ ·ä¾‹
        æ­¤åœºæ™¯é€‚ç”¨äºæ‚¨æ²¡æœ‰ä»»ä½•è®­ç»ƒæ•°æ®ï¼Œä¹Ÿä¸å¸Œæœ›ç”Ÿæˆåˆæˆæ ·ä¾‹çš„æƒ…å†µã€‚ç³»ç»Ÿå°†ç›´æ¥ä¼˜åŒ–æç¤ºè¯ã€‚
        
        ### åœºæ™¯2ï¼šæ²¡æœ‰è®­ç»ƒæ•°æ®ï¼Œä½¿ç”¨åˆæˆæ ·ä¾‹
        æ­¤åœºæ™¯é€‚ç”¨äºæ‚¨æ²¡æœ‰è®­ç»ƒæ•°æ®ï¼Œä½†å¸Œæœ›ä½¿ç”¨åˆæˆæ ·ä¾‹çš„æƒ…å†µã€‚ç³»ç»Ÿå°†é¦–å…ˆç”Ÿæˆåˆæˆæ ·ä¾‹ï¼Œç„¶ååŸºäºè¿™äº›æ ·ä¾‹ä¼˜åŒ–æç¤ºè¯ã€‚
        
        ### åœºæ™¯3ï¼šæœ‰è®­ç»ƒæ•°æ®ï¼Œä½¿ç”¨æ ·ä¾‹
        æ­¤åœºæ™¯é€‚ç”¨äºæ‚¨å·²æœ‰è®­ç»ƒæ•°æ®ï¼Œå¹¶å¸Œæœ›ä½¿ç”¨è¿™äº›æ•°æ®è¿›è¡Œæç¤ºè¯ä¼˜åŒ–çš„æƒ…å†µã€‚
        
        ### å‚æ•°è¯´æ˜
        - **ä»»åŠ¡æè¿°**ï¼šæè¿°è¦æ‰§è¡Œçš„ä»»åŠ¡
        - **åŸºç¡€æŒ‡ä»¤**ï¼šæä¾›ç»™æ¨¡å‹çš„åŸºæœ¬æŒ‡ä»¤
        - **å˜å¼‚è½®æ¬¡**ï¼šæŒ‡å®šæç¤ºè¯å˜å¼‚çš„è½®æ•°
        - **ç”Ÿæˆä¸“å®¶èº«ä»½**ï¼šæ˜¯å¦ç”Ÿæˆä¸“å®¶èº«ä»½
        - **å°‘æ ·æœ¬æ•°é‡**ï¼šåœ¨æç¤ºè¯ä¸­ä½¿ç”¨çš„ç¤ºä¾‹æ•°é‡
        
        ### åŠŸèƒ½æŒ‰é’®
        - **åˆ›å»ºåˆæˆæ•°æ®**ï¼šåœ¨åœºæ™¯2ä¸­ï¼Œç”¨äºç”Ÿæˆåˆæˆè®­ç»ƒæ•°æ®
        - **è¿è¡Œåœºæ™¯X**ï¼šæ‰§è¡Œç›¸åº”åœºæ™¯çš„æç¤ºè¯ä¼˜åŒ–
        - **ä¿å­˜XXé…ç½®**ï¼šä¿å­˜å¯¹åº”çš„é…ç½®æ–‡ä»¶
        """)

    # è¿è¡Œæ—¥å¿—å’Œç‰ˆæœ¬ä¿¡æ¯
    gr.Markdown("### ğŸŒŸ PromptWizard v1.0.0 | æç¤ºè¯ä¼˜åŒ–ç³»ç»Ÿ")

if __name__ == "__main__":
    # åˆ›å»ºresultsç›®å½•
    if not os.path.exists("results"):
        os.makedirs("results")
    
    # åŠ è½½ç¯å¢ƒå˜é‡
    load_dotenv(override=True)
    # å¯åŠ¨Gradioç•Œé¢
    demo.launch(server_port=7860) 